{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXYIqvrW7u7c"
      },
      "outputs": [],
      "source": [
        "# load libraries\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import tensorflow as tf\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import umap\n",
        "\n",
        "from tensorflow.python.data.ops import dataset_ops\n",
        "from tensorflow.python.keras.layers.preprocessing import image_preprocessing\n",
        "from tensorflow.python.keras.preprocessing import dataset_utils\n",
        "from tensorflow.python.ops import image_ops\n",
        "from tensorflow.python.ops import io_ops\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score, silhouette_score, calinski_harabasz_score\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.optimize import linear_sum_assignment as linear_assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code, taken from GitHub, allows you to match predictions with file names."
      ],
      "metadata": {
        "id": "utecs_am762o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WHITELIST_FORMATS = ('.bmp', '.gif', '.jpeg', '.jpg', '.png')\n",
        "\n",
        "# Tensorflow override method to return fname as list as well as dataset\n",
        "def image_dataset_from_directory(directory,\n",
        "                                 labels='inferred',\n",
        "                                 label_mode='int',\n",
        "                                 class_names=None,\n",
        "                                 color_mode='rgb',\n",
        "                                 batch_size=32,\n",
        "                                 image_size=(256, 256),\n",
        "                                 shuffle=True,\n",
        "                                 seed=None,\n",
        "                                 validation_split=None,\n",
        "                                 subset=None,\n",
        "                                 interpolation='bilinear',\n",
        "                                 follow_links=False):\n",
        "  \n",
        "  if labels != 'inferred':\n",
        "    if not isinstance(labels, (list, tuple)):\n",
        "      raise ValueError(\n",
        "          '`labels` argument should be a list/tuple of integer labels, of '\n",
        "          'the same size as the number of image files in the target '\n",
        "          'directory. If you wish to infer the labels from the subdirectory '\n",
        "          'names in the target directory, pass `labels=\"inferred\"`. '\n",
        "          'If you wish to get a dataset that only contains images '\n",
        "          '(no labels), pass `label_mode=None`.')\n",
        "    if class_names:\n",
        "      raise ValueError('You can only pass `class_names` if the labels are '\n",
        "                       'inferred from the subdirectory names in the target '\n",
        "                       'directory (`labels=\"inferred\"`).')\n",
        "  if label_mode not in {'int', 'categorical', 'binary', None}:\n",
        "    raise ValueError(\n",
        "        '`label_mode` argument must be one of \"int\", \"categorical\", \"binary\", '\n",
        "        'or None. Received: %s' % (label_mode,))\n",
        "  if color_mode == 'rgb':\n",
        "    num_channels = 3\n",
        "  elif color_mode == 'rgba':\n",
        "    num_channels = 4\n",
        "  elif color_mode == 'grayscale':\n",
        "    num_channels = 1\n",
        "  else:\n",
        "    raise ValueError(\n",
        "        '`color_mode` must be one of {\"rbg\", \"rgba\", \"grayscale\"}. '\n",
        "        'Received: %s' % (color_mode,))\n",
        "  interpolation = image_preprocessing.get_interpolation(interpolation)\n",
        "  dataset_utils.check_validation_split_arg(\n",
        "      validation_split, subset, shuffle, seed)\n",
        "\n",
        "  if seed is None:\n",
        "    seed = np.random.randint(1e6)\n",
        "  image_paths, labels, class_names = dataset_utils.index_directory(\n",
        "      directory,\n",
        "      labels,\n",
        "      formats=WHITELIST_FORMATS,\n",
        "      class_names=class_names,\n",
        "      shuffle=shuffle,\n",
        "      seed=seed,\n",
        "      follow_links=follow_links)\n",
        "\n",
        "  if label_mode == 'binary' and len(class_names) != 2:\n",
        "    raise ValueError(\n",
        "        'When passing `label_mode=\"binary\", there must exactly 2 classes. '\n",
        "        'Found the following classes: %s' % (class_names,))\n",
        "\n",
        "  image_paths, labels = dataset_utils.get_training_or_validation_split(\n",
        "      image_paths, labels, validation_split, subset)\n",
        "\n",
        "  dataset = paths_and_labels_to_dataset(\n",
        "      image_paths=image_paths,\n",
        "      image_size=image_size,\n",
        "      num_channels=num_channels,\n",
        "      labels=labels,\n",
        "      label_mode=label_mode,\n",
        "      num_classes=len(class_names),\n",
        "      interpolation=interpolation)\n",
        "  #if shuffle:\n",
        "    # Shuffle locally at each iteration\n",
        "  #  dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  # Users may need to reference `class_names`.\n",
        "  dataset.class_names = class_names\n",
        "  return dataset, image_paths\n",
        "\n",
        "def paths_and_labels_to_dataset(image_paths,\n",
        "                                image_size,\n",
        "                                num_channels,\n",
        "                                labels,\n",
        "                                label_mode,\n",
        "                                num_classes,\n",
        "                                interpolation):\n",
        "  \"\"\"Constructs a dataset of images and labels.\"\"\"\n",
        "  # TODO(fchollet): consider making num_parallel_calls settable\n",
        "  path_ds = dataset_ops.Dataset.from_tensor_slices(image_paths)\n",
        "  img_ds = path_ds.map(\n",
        "      lambda x: path_to_image(x, image_size, num_channels, interpolation))\n",
        "  if label_mode:\n",
        "    label_ds = dataset_utils.labels_to_dataset(labels, label_mode, num_classes)\n",
        "    img_ds = dataset_ops.Dataset.zip((img_ds, label_ds))\n",
        "  return img_ds\n",
        "\n",
        "\n",
        "def path_to_image(path, image_size, num_channels, interpolation):\n",
        "  img = io_ops.read_file(path)\n",
        "  img = image_ops.decode_image(\n",
        "      img, channels=num_channels, expand_animations=False)\n",
        "  img = image_ops.resize_images_v2(img, image_size, method=interpolation)\n",
        "  img.set_shape((image_size[0], image_size[1], num_channels))\n",
        "  return img"
      ],
      "metadata": {
        "id": "nr3lrEkH7404"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following ad-hoc function calculates the \"clustering\" accuracy."
      ],
      "metadata": {
        "id": "nyE2wXVa8RZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# auxiliary function\n",
        "def accuracy(y_true, y_pred):\n",
        "    y_true = y_true.astype(np.int64)\n",
        "    assert y_pred.size == y_true.size\n",
        "    D = max(y_pred.max(), y_true.max()) + 1\n",
        "    w = np.zeros((D, D), dtype=np.int64)\n",
        "    \n",
        "    for i in range(y_pred.size):\n",
        "        w[y_pred[i], y_true[i]] += 1\n",
        "    row_ind, col_ind = linear_assignment(w.max() - w)\n",
        "    \n",
        "    return w[row_ind, col_ind].sum() * 1.0 / y_pred.size"
      ],
      "metadata": {
        "id": "S-4Y7EKE8QDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "data_dir = 'PATH_TO_DATA'\n",
        "\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)\n",
        "\n",
        "batch_size = 32\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "\n",
        "ds, paths = image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,  \n",
        "    label_mode='int')\n",
        "\n",
        "class_names = ds.class_names\n",
        "print(class_names)\n",
        "\n",
        "# display a few images\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in ds.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        print(paths[i])\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "QSaB3MzD8acP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code extracts features from a pre-trained model."
      ],
      "metadata": {
        "id": "8lRJJjl79jlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_input = tf.keras.applications.densenet.preprocess_input\n",
        "\n",
        "base_model = tf.keras.applications.DenseNet121(input_shape=(224, 224, 3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet',\n",
        "                                               pooling='avg')\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "x = preprocess_input(inputs)\n",
        "outputs = base_model(x, training=False)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "features = []\n",
        "labels =  np.array([])\n",
        "for x, y in ds.take(247): # the input argument decides the number of data batches you want to consider, e.g. 247\n",
        "    features.append(model.predict(x))\n",
        "    labels = np.concatenate([labels, y])\n",
        "\n",
        "features = np.asarray(features).reshape(247*32, 1024)"
      ],
      "metadata": {
        "id": "dbeiAbAF8owj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following snippet defines and pre-trains a simple fully-connected autoencoder."
      ],
      "metadata": {
        "id": "d5phwyyT-Sf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
        "    n_stacks = len(dims) - 1\n",
        "    # input\n",
        "    input_img = tf.keras.layers.Input(shape=(dims[0],), name='input')\n",
        "    x = input_img\n",
        "    # internal layers of the encoder\n",
        "    for i in range(n_stacks-1):\n",
        "        x = tf.keras.layers.Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n",
        "\n",
        "    # bottleneck\n",
        "    encoded = tf.keras.layers.Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)  # hidden layer, features are extracted from here\n",
        "\n",
        "    x = encoded\n",
        "    # hidden layers of the decoder\n",
        "    for i in range(n_stacks-1, 0, -1):\n",
        "        x = tf.keras.layers.Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n",
        "\n",
        "    # output\n",
        "    x = tf.keras.layers.Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
        "    decoded = x\n",
        "    return tf.keras.models.Model(inputs=input_img, outputs=decoded, name='AE'), tf.keras.models.Model(inputs=input_img, outputs=encoded, name='encoder')\n",
        "\n",
        "dims = [1024, 500, 500, 2000, 10]\n",
        "init = tf.keras.initializers.VarianceScaling(scale=1. / 3., mode='fan_in',\n",
        "                                             distribution='uniform')\n",
        "autoencoder, encoder = autoencoder(dims, init=init)\n",
        "autoencoder.summary()\n",
        "\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "autoencoder.fit(features, features, batch_size=256, epochs=200)\n",
        "\n",
        "# save model\n",
        "save_dir = 'PATH_TO_MODELS'\n",
        "autoencoder.save(save_dir + 'pretrained_autoencoder.h5')"
      ],
      "metadata": {
        "id": "LDyKS6QS-OqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The following is the original implementation of the deep clustering layer."
      ],
      "metadata": {
        "id": "1I-O-VTU-v6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = 10 # number of clusters to be considered\n",
        "\n",
        "class ClusteringLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
        "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
        "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
        "        super(ClusteringLayer, self).__init__(**kwargs)\n",
        "        self.n_clusters = n_clusters\n",
        "        self.alpha = alpha\n",
        "        self.initial_weights = weights\n",
        "        self.input_spec = tf.keras.layers.InputSpec(ndim=2)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 2\n",
        "        input_dim = input_shape[1]\n",
        "        self.input_spec = tf.keras.layers.InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
        "        self.clusters = self.add_weight('clusters', (self.n_clusters, input_dim), initializer='glorot_uniform')\n",
        "        if self.initial_weights is not None:\n",
        "            self.set_weights(self.initial_weights)\n",
        "            del self.initial_weights\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
        "        q **= (self.alpha + 1.0) / 2.0\n",
        "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure each sample's 10 values add up to 1.\n",
        "        return q\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert input_shape and len(input_shape) == 2\n",
        "        return input_shape[0], self.n_clusters\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'n_clusters': self.n_clusters}\n",
        "        base_config = super(ClusteringLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
        "model = tf.keras.models.Model(inputs=encoder.input, outputs=clustering_layer)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "l8GugDrJ-ktr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following snippet trains the model and shows its performance."
      ],
      "metadata": {
        "id": "oNLZFLC5_oZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='kld')\n",
        "\n",
        "# initialize cluster centroids using k-means\n",
        "kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
        "y_pred = kmeans.fit_predict(encoder.predict(features))\n",
        "\n",
        "y_pred_last = np.copy(y_pred)\n",
        "model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
        "\n",
        "# compute auxiliary target distribution\n",
        "def target_distribution(q):\n",
        "    weight = q ** 2 / q.sum(0)\n",
        "    return (weight.T / weight.sum(1)).T\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 128\n",
        "loss = 0\n",
        "index = 0\n",
        "maxiter = 8000\n",
        "update_interval = 140\n",
        "index_array = np.arange(features.shape[0])\n",
        "tol = 0.001 # tolerance threshold to stop training\n",
        "\n",
        "for ite in range(int(maxiter)):\n",
        "    if ite % update_interval == 0:\n",
        "        q = model.predict(features, verbose=0)\n",
        "        p = target_distribution(q)  # update the auxiliary target distribution\n",
        "        embedded_features = encoder.predict(features) # embeddings\n",
        "        \n",
        "        # evaluate clustering performance\n",
        "        y_pred = q.argmax(1)\n",
        "        if labels is not None:\n",
        "            acc = np.round(accuracy(labels, y_pred), 5)\n",
        "            nmi = np.round(normalized_mutual_info_score(labels, y_pred), 5)\n",
        "            ars = np.round(adjusted_rand_score(labels, y_pred), 5)\n",
        "            ss = np.round(silhouette_score(embedded_features, y_pred), 5)\n",
        "            chs = np.round(calinski_harabasz_score(embedded_features, y_pred), 5)\n",
        "            loss = np.round(loss, 5)\n",
        "            print('Iter %d: acc = %.5f, nmi = %.5f, ars = %.5f, ss = %.5f, chs = %.5f' % (ite, acc, nmi, ars, ss, chs), '; loss=', loss)\n",
        "\n",
        "        # check stopping criterion\n",
        "        delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
        "        y_pred_last = np.copy(y_pred)\n",
        "        if ite > 0 and delta_label < tol:\n",
        "            print('delta_label ', delta_label, '< tol ', tol)\n",
        "            print('Reached tolerance threshold. Stopping training.')\n",
        "            break\n",
        "            \n",
        "    idx = index_array[index * batch_size: min((index+1) * batch_size, features.shape[0])]\n",
        "    loss = model.train_on_batch(x=features[idx], y=p[idx])\n",
        "    index = index + 1 if (index + 1) * batch_size <= features.shape[0] else 0"
      ],
      "metadata": {
        "id": "Gw9GxtGv-84u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last part of the code is responsible for the qualitative evaluation of the clusters found."
      ],
      "metadata": {
        "id": "nc9Yw5VD_e7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# t-SNE visualization\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_embedding = tsne.fit_transform(embedded_features)\n",
        "\n",
        "plt.figure(dpi=600)\n",
        "plt.scatter(*zip(*tsne_embedding[:,:2]), c=y_pred, cmap='viridis')\n",
        "plt.tick_params(labelsize=15)\n",
        "#plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# UMAP visualization\n",
        "reducer = umap.UMAP()\n",
        "umap_embedding = reducer.fit_transform(embedded_features)\n",
        "\n",
        "plt.figure(dpi=600)\n",
        "plt.scatter(*zip(*umap_embedding[:,:2]), c=y_pred, cmap='viridis')\n",
        "plt.tick_params(labelsize=15)\n",
        "#plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2Jkhc-RH_Rx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_paths = paths\n",
        "\n",
        "sample_df = pd.DataFrame(sample_paths, columns=['path'])\n",
        "y_pred_df = pd.DataFrame(y_pred, columns=['y_pred'])\n",
        "df = pd.concat([sample_df, y_pred_df], axis=1)\n",
        "\n",
        "cluster = 1 # clyster to be checked\n",
        "sample = df[df['y_pred']==cluster].sample(n=50, random_state=42)\n",
        "\n",
        "# display a few images from that cluster\n",
        "for index, row in sample.iterrows():\n",
        "    print(row['path'])\n",
        "    img = mpimg.imread(row['path'])\n",
        "    imgplot = plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "EddmdEQv_eAp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}